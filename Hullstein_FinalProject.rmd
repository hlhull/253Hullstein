---
title: "Hullstein_FinalProject"
author: "Eleanor Wettstein"
date: "4/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r, echo=FALSE}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(GGally) #for nice scatterplot matrix 
library(ggridges) #for joy/ridge plots
library(corrplot) #for basic correlation matrix plot
library(naniar) #for exploring missing values
library(pdp) #for partial dependence plots, MARS models
library(rpart.plot) #for plotting decision trees
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(lubridate) #for nice dates
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data
library(moderndive) #for data

#modeling
library(rsample) #for splitting data
library(recipes) #for keeping track of transformations
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(earth) #for MARS models
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests

library(e1071)

theme_set(theme_minimal())
```

## READ IN DATA / CLEAN

```{r}
# read in data, replace ? with NA
diabetes <- read_csv("diabetic_data.csv", na = "?")

# remove people who died who were discharged to hospice
diabetes <- diabetes[!(diabetes$discharge_disposition_id==19 |
                         diabetes$discharge_disposition_id==21 |
                         diabetes$discharge_disposition_id==13 |
                         diabetes$discharge_disposition_id==14 |
                         diabetes$discharge_disposition_id==11 |
                         diabetes$discharge_disposition_id==20),]

# make classifications categories
diabetes$discharge_disposition_id <- as.factor(diabetes$discharge_disposition_id)
diabetes$admission_type_id <- as.factor(diabetes$admission_type_id)
diabetes$admission_source_id <- as.factor(diabetes$admission_source_id)

# lump discharge into 4 categories
diabetes <- diabetes %>%
  mutate(discharge_disposition_id = fct_lump(discharge_disposition_id,n=3))

# remove medicines without 2+ categories, categories with lots missing, and other variables
drop <- c("acetohexamide", "tolbutamide", "troglitazone", "examide", "citoglipton", "glyburide-metformin", "glipizide-metformin", "glimepiride-pioglitazone", "metformin-rosiglitazone", "metformin-pioglitazone", "weight", "medical_specialty", "payer_code", "encounter_id", "patient_nbr", "diag_1", "diag_2", "diag_3")
diabetes <- diabetes[, !(names(diabetes) %in% drop)]

# make binomial readmitted variable
diabetes <- diabetes %>% 
  mutate(readmitted30 = ifelse(readmitted == "<30", 'true', 'false'))
```

```{r}
# split into test and train
set.seed(253) 
diab_split <- initial_split(diabetes, prop = .7, 
                             strata = readmitted30)
diab_train <- training(diab_split)
diab_test <- testing(diab_split)

head(diab_train)
```
## GRAPHS
TO DO: Introduce your data and research question. It might be helpful to include a graph/table that summarizes the response variable. Lay out the plan you will use to analyze/model so we (the readers!) can more easily follow along and knows what to expect. It can be helpful to elude to some of the results at the beginning so we know what to watch for. Also mention any important data cleaning decisions you made. You don’t need to tell us EVERYTHING you do, though.

```{r}
dim(diab_train)
glimpse(diab_train)
```


```{r}
ggplot(diabetes, aes(x=readmitted)) +
  geom_bar()

ggplot(diabetes, aes(x=age, fill=readmitted30)) +
  geom_bar(position="fill")

ggplot(diabetes, aes(x=num_lab_procedures)) +
  geom_histogram()
```

```{r}
diabetes %>% 
  ggplot(aes(x = age, fill = readmitted30)) +
  geom_bar(position = "fill")

diabetes %>% 
  ggplot(aes(x = readmitted30, fill = age)) +
  geom_bar(position = "fill")

diabetes %>% 
  ggplot(aes(x = readmitted30, fill = A1Cresult)) +
  geom_bar(position = "fill")

diabetes %>% 
  ggplot(aes(x = readmitted30, fill = factor(time_in_hospital))) +
  geom_bar(position = "fill")
diabetes %>% 
  ggplot(aes(x = factor(time_in_hospital), fill = readmitted30)) +
  geom_bar(position = "fill")

ggplot(diabetes, aes(x=time_in_hospital, fill=readmitted30)) +
  geom_density(alpha=.5)
```

```{r}
ggplot(diabetes, aes(x=num_lab_procedures, fill=readmitted30)) +
  geom_density(alpha=.5)
```


## MODELS

TO DO: Exploratory work and basic models. Use graphs to illustrate interesting relationshipos that play a role in your final model. DO NOT just show a bunch of graphs because you can. You should label and discuss every graph you include. There is no required number to include. The graphs should be helping us to understand something about your final model and should help us engage more with the data.


# LOGISTIC REGRESSION 

```{r}
other_vars <- c('race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id','time_in_hospital','num_lab_procedures','num_procedures', 'num_medications', 'readmitted30')
other_subset <- diab_train[other_vars]


med_vars <- c('metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide','glyburide','pioglitazone','rosiglitazone','acarbose','miglitol','tolazamide',
  'insulin', 'readmitted30')
med_subset <- diab_train[med_vars]

med_subsetS <- med_subset %>%
  slice(1:3000)
```


```{r}
# Set the seed
set.seed(253)

# Run the model
diabO_logAll <- train(
    readmitted30 ~ .,
    data = other_subset, 
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
summary(diabO_logAll)
diabO_logAll$results$Accuracy
```

```{r}
# Set the seed
set.seed(253)

# Run the model
diabM_logAll <- train(
    readmitted30 ~ .,
    data = med_subset, 
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
summary(diabM_logAll)
diabM_logAll$results$Accuracy
```

# LASSO
```{r}
set.seed(253)

diabO_lasso <- train(
    readmitted30 ~ .,
    data = other_subset,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 10^seq(-3, -1, length = 100)),
    metric = "Accuracy",
    na.action = na.omit
)

diabO_lasso$bestTune
```
```{r}
diabO_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line() +
  geom_point() +
  scale_x_log10()
```


```{r}
set.seed(253)

#lambda_grid <- 10^seq(-4, -2, length = 100)

diabM_lasso <- train(
    readmitted30 ~ .,
    data = med_subset,
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 10^seq(-4, -1, length = 100)),
    metric = "Accuracy",
    na.action = na.omit
)

diabM_lasso$bestTune
```

```{r}
diabM_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line() +
  geom_point() +
  scale_x_log10()

coefficients(diabM_lasso$finalModel, s = diabM_lasso$bestTune$lambda)
```

# classification tree
```{r}
set.seed(253)

diabO_tree <- train(
  readmitted30 ~ .,
  data = other_subset,
  method = "rpart",
  tuneGrid = data.frame(cp = 10^seq(-4, -3 , 
                                    length = 50)),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
diabO_tree$bestTune
diabO_tree

diabO_tree$results %>%
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_line()+
  geom_point()
```


```{r}
set.seed(253)

diabM_tree <- train(
  readmitted30 ~ .,
  data = med_subset,
  method = "rpart",
  tuneGrid = data.frame(cp = 10^seq(-4, -2, 
                                    length = 50)),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
diabM_tree$results %>%
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_line()+
  geom_point()
```


# random forest

```{r}
set.seed(327)
diabM_randf <- train(
  readmitted30 ~ .,
  data = med_subsetS %>% select(-miglitol, -nateglinide), 
  method = "rf",
  metric = "Accuracy",
  trControl = trainControl(method = "oob"),
  tuneGrid = data.frame(mtry = c(4,6,8,10,12,14)),
  ntree = 100, #number of trees used, default is 500
  importance = TRUE, #for importance plots later
  nodesize = 5, #this is the default terminal node size for regression trees. Could set larger for smaller trees.
  na.action = na.omit
)
```


```{r}
plot(diabM_randf$finalModel)
diabM_randf$results
```

```{r}
vip(diabM_randf$finalModel)

```

## PROCESS

TO DO: Describe the modeling process. I don’t want to know EVERYTHING you tried. You might have tried 10 different things and in the end chose the 8th one. If you think it is important, you can summarize some other methods you tried. But focus on the analysis you found to be most important. This should include at least one of the techniques you learned in this course. Some essentials you will need in the modeling process:

  Use rsample() to split the data into a training and test set.
  Tune the model using the training data.
  Evaluate model using training data (cross-validation or OOB).
  Pick a few “best” models and apply them to the test data to decide on the final model.

## SUMMARY

Summarize the results succinctly. Reiterate why we should be interested in this analysis. Depending on the project, you might do this in different ways. Maybe there were important relationships you learned about. Or maybe you have a nice way to predict something useful. Let us know what we’ve learned and why it’s important/neat/interesting.





