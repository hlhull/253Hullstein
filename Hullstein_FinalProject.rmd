---
title: "Hullstein Final Project: Analyzing Diabetes Readmittance"
author: "Holly Hull and Eleanor Wettstein"
date: "5/7/2020"
output: 
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

```{r}
#plotting and exploring
library(tidyverse) #for plotting and summarizing
library(vip) #for importance plots
library(pROC) #for ROC curves
library(plotROC) #for plotting ROC curves

#making things look nice
library(knitr) #for nice tables
library(scales) #for nice labels on graphs
library(gridExtra) #for arranging plots
library(broom) #for nice model output
library(janitor) #for nice names

#data
library(ISLR) #for data

#modeling
library(rsample) #for splitting data
library(caret) #for modeling
library(leaps) #for variable selection
library(glmnet) #for LASSO
library(rpart) #for decision trees
library(randomForest) #for bagging and random forests
library(rpart.plot)

library(e1071)

theme_set(theme_minimal())
```

# Introduction

```{r}
# read in data, replace ? with NA
diabetes <- read_csv("diabetic_data.csv", na = "?")

# remove people who died who were discharged to hospice
diabetes <- diabetes[!(diabetes$discharge_disposition_id==19 |
                         diabetes$discharge_disposition_id==21 |
                         diabetes$discharge_disposition_id==13 |
                         diabetes$discharge_disposition_id==14 |
                         diabetes$discharge_disposition_id==11 |
                         diabetes$discharge_disposition_id==20),]

# make classifications categories
diabetes$discharge_disposition_id <- as.factor(diabetes$discharge_disposition_id)
diabetes$admission_type_id <- as.factor(diabetes$admission_type_id)
diabetes$admission_source_id <- as.factor(diabetes$admission_source_id)

# lump discharge into 4 categories (home, skilled nursing facility, home health service, other)
diabetes <- diabetes %>%
  mutate(discharge_disposition_id = fct_lump(discharge_disposition_id,n=3))

# lump admission source into 3 categories (physician, emergency room, other)
diabetes <- diabetes %>%
  mutate(admission_source_id = fct_lump(admission_source_id,n=2))

# lump admission type into 4 categories (Emergency, Urgent, Elective, other)
diabetes <- diabetes %>%
  mutate(admission_type_id = fct_lump(admission_type_id,n=3))

# combine youngest ages
diabetes$age <- fct_collapse(diabetes$age, "[0-30)"= c("[0-10)","[10-20)","[20-30)"))

# remove medicines without 2+ categories, categories with lots missing, and other variables
drop <- c("acetohexamide", "tolbutamide", "troglitazone", "examide", "citoglipton", "glyburide-metformin", "glipizide-metformin", "glimepiride-pioglitazone", "metformin-rosiglitazone", "metformin-pioglitazone", "weight", "medical_specialty", "payer_code", "encounter_id", "patient_nbr", "diag_1", "diag_2", "diag_3")
diabetes <- diabetes[, !(names(diabetes) %in% drop)]

# make binomial readmitted variable
diabetes <- diabetes %>% 
  mutate(readmitted30 = ifelse(readmitted == "<30", 'true', 'false'))
diabetes$readmitted30 <- as.factor(diabetes$readmitted30)

# remove NA rows
diabetes <- na.omit(diabetes)
```

```{r}
# split into test and train
set.seed(253) 
diab_split <- initial_split(diabetes, prop = .7, 
                             strata = readmitted30)
diab_train <- training(diab_split)
diab_test <- testing(diab_split)
```

Background: It is essential to provide the proper care for patients with diabetes, especially those that are hospitalized, in order to achieve the optimum outcome in terms of mortality and overall level of health and well-being. One way to approach this is to keep track of what factors contribute to a diabetes patient being readmitted to a hospital following previous treatment.

Data: We used a clinical database that contains information about patients who sought or required medical care due to diabetes. This dataset included variables such as demographic descriptors, type and severity of diabetes, and type of medical visit. 

Research question: What variables can be used to best predict whether or not a diabetes patient will be readmitted to a medical facility within 30 days of previous treatment. 

Plan of action: Using modeling techniques that we have learned in class, we plan to apply various models to the dataset (such as logistic regression and lasso) in an effort to determine which predictors are most useful in assessing whether a patient will be readmitted to a hospital.

Data cleaning: We performed some data cleaning before testing the models.

* If predictor variables had a few terms that constituted the majority of observations, we grouped the more sparse terms into an “other” category (or an equivalent grouping that made sense). This was done for `discharge_disposition_id`, `admission_type_id`, `admission_source_id`, and `age`.

* We removed observations corresponding to patients who died or were placed on hospice. While they technically were not readmitted to a hospital within 30 days, they are not healthy and would skew that data.

* We removed variables that had majority missing values or for which most of the observations had the same term. This resulted in the dropping of `weight`, several medications, and a few other variables.

* Finally, we created our response variable, `readmitted30`, to be a binomial variable to allow for logistic regression analysis. This variable is `true` if the patient is readmitted into the hospital within 30 days of being discharged, and `false` otherwise.


The following is a graph summarizing the response variable, `readmitted30`.

```{r}
ggplot(diabetes, aes(x=readmitted30)) +
  geom_bar()
```

As you can see, most patients were not readmitted within 30 days. Because of this imbalance, we had to down sample our models so that they didn't end up reflecting the No Information Rate (NIR). Instead, we would rather have more false positives, so that doctors are aware of patients who may be at a higher risk of being readmitted.

# Exploratory Analysis

```{r, results = 'hide'}
# table(diab_train$readmitted30) %>% prop.table()
# dim(diab_train)
# glimpse(diab_train)
```


```{r, results = 'hide'}
# ggplot(diabetes, aes(x=readmitted30)) +
#   geom_bar()
# 
# ggplot(diabetes, aes(x=num_lab_procedures, fill = readmitted30)) +
#   geom_histogram()
```

```{r, results = 'hide'}

# ggplot(diabetes, aes(x=age, fill=readmitted30)) +
#   geom_bar(position="fill")
# 
# diabetes %>% 
#   ggplot(aes(x = age, fill = readmitted30)) +
#   geom_bar(position = "fill")
# 
# diabetes %>% 
#   ggplot(aes(x = readmitted30, fill = age)) +
#   geom_bar(position = "fill")
# 
# diabetes %>% 
#   ggplot(aes(x = readmitted30, fill = A1Cresult)) +
#   geom_bar(position = "fill")
# 
# diabetes %>% 
#   ggplot(aes(x = readmitted30, fill = factor(time_in_hospital))) +
#   geom_bar(position = "fill")
# diabetes %>% 
#   ggplot(aes(x = factor(time_in_hospital), fill = readmitted30)) +
#   geom_bar(position = "fill")
# 
# ggplot(diabetes, aes(x=time_in_hospital, fill=readmitted30)) +
#   geom_density(alpha=.5)
```

```{r}
# combine infrequent #s in top range
diabetes$number_inpatient_graph <- as.factor(diabetes$number_inpatient)
diabetes$number_inpatient_graph <- fct_collapse(diabetes$number_inpatient_graph, "11+"= c("11","12","13", "14", "15", "16", "17","18","19","20","21"))

diabetes %>% 
  ggplot(aes(x = number_inpatient_graph, fill = readmitted30)) +
  geom_bar(position = "fill") +
  labs(x = "number_inpatient")
```

This shows the number of inpatient visits the patient had the year before the hospitalization. This becomes one of the most important variables in the model because, as you can see in the graph, as the number of inpatient visits the patient had increases, the likelihood that they will be readmitted also increases.

```{r}
diabetes %>% 
  ggplot(aes(x = discharge_disposition_id, fill = readmitted30)) +
  geom_bar(position = "fill")
```

The `discharge_disposition_id` became another important variable. This variable describes where the patient went after they were released from the hosptial. '1' means they went home, which you can see has the smallest proportion of readmitted patients. '3' means they went to a skilled nursing facility and '6' means they went home, but also had a health service at home to assist them. 

```{r}
# combine infrequent #s in top range
diabetes$number_diagnoses_graph <- as.factor(diabetes$number_diagnoses)
diabetes$number_diagnoses_graph <- fct_collapse(diabetes$number_diagnoses_graph, "9+"= c("9", "10", "11","12","13", "14", "15", "16"))

diabetes %>% 
  ggplot(aes(x = number_diagnoses_graph, fill = readmitted30)) +
  geom_bar(position = "fill") +
  labs(x = "number_diagnoses")
```

Finally, `number_diagnoses` was one last key variable. It describes how many diagnoses were entered into the patients record during the hospital visit. As the number of diagnoses increases, so does the proportion of patients readmitted.

# Models

```{r}
calcAUC <- function(model, data){
  newData <- na.omit(data) %>% 
    mutate(PredProb = predict(model, newdata = data, type="prob")$"true")
  newData %>% 
    roc(readmitted30, PredProb) %>% 
    auc()
}
```


## Logistic Regression 

```{r, results = 'hide'}
# other_vars <- c('race', 'gender', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id','time_in_hospital','num_lab_procedures','num_procedures', 'num_medications', 'readmitted30')
# other_subset <- diab_train[other_vars]
# 
# 
# med_vars <- c('metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide','glyburide','pioglitazone','rosiglitazone','acarbose','miglitol','tolazamide',
#   'insulin', 'readmitted30')
# med_subset <- diab_train[med_vars]
# 
# med_subsetS <- med_subset %>%
#   slice(1:3000)
```


```{r, results = 'hide'}
# # Set the seed
# set.seed(253)
# 
# # Run the model
# diabO_logAll <- train(
#     readmitted30 ~ .,
#     data = other_subset, 
#     method = "glm",
#     family = "binomial",
#     trControl = trainControl(method = "cv", number = 5, sampling = "down"),
#     metric = "Accuracy",
#     na.action = na.omit
# )
```

```{r, results = 'hide'}
# summary(diabO_logAll$finalModel) %>% 
#   coef() %>% 
#   tidy() %>% 
#   select(`.rownames`, Estimate) %>% 
#   mutate(exp_coef = exp(Estimate))
# diabO_logAll$results$Accuracy

#confusionMatrix(data = predict(diabO_logAll, type = "raw"), #predictions
#                reference = other_subset$readmitted30, #actuals
#                positive = "true") 
```

```{r, echo = TRUE}
# Set the seed
set.seed(253)

# Run the model
diab_logAll <- train(
    readmitted30 ~ .,
    data = diab_train %>% select(-readmitted), 
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    metric = "Accuracy",
    na.action = na.omit
)
```

```{r}
diab_logAll$results$Accuracy

calcAUC(diab_logAll, diab_train)

cm_logAll <- confusionMatrix(data = predict(diab_logAll, type = "raw", na.action = na.pass), #predictions
                reference = diab_train$readmitted30)#, #actuals

cm_logAll$byClass['Sensitivity']
```

For our preliminary model, we performed a logistic regression using all the predictor variables. We computed the accuracy, sensitivity, and area under the curve (AUC), which are statistics that give insight about how well our models perform. We were particularly interested in the sensitivity, or True Positive Rate (TPR), which describes the proportion of patients who were correctly predicted to be readmitted within 30 days. Our goal is to maximize this value, so that medical practitioners will gain insight about which patients should be monitored more closely after they are discharged. For this model, the sensitivity is 0.6569. The accuracy and AUC are 0.6423 and 0.6583, respectively, and are summarized in Table 1. 


## Lasso
```{r, results = 'hide'}
# set.seed(253)
# 
# diabO_lasso <- train(
#     readmitted30 ~ .,
#     data = other_subset,
#     method = "glmnet",
#     family = "binomial",
#     trControl = trainControl(method = "cv", number = 5, sampling = "down"),
#     tuneGrid = data.frame(alpha = 1, 
#                           lambda = 10^seq(-5, -1, length = 100)),
#     metric = "Accuracy",
#     na.action = na.omit
# )
# 
# diabO_lasso$bestTune
```
```{r, results = 'hide'}
# diabO_lasso$results %>% 
#   ggplot(aes(x = lambda, y = Accuracy)) +
#   geom_line() +
#   geom_point() +
#   scale_x_log10()
```


```{r, echo = TRUE}
set.seed(253)

diab_lasso <- train(
    readmitted30 ~ .,
    data = diab_train %>% select(-readmitted),
    method = "glmnet",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5, sampling = "down"),
    tuneGrid = data.frame(alpha = 1, 
                          lambda = 10^seq(-4, -1, length = 100)),
    metric = "Accuracy",
    na.action = na.omit
)

```

```{r}
diab_lasso$results %>% 
  ggplot(aes(x = lambda, y = Accuracy)) +
  geom_line() +
  geom_point() +
  scale_x_log10()

diab_lasso$bestTune$lambda
diab_lasso$results[85,]

calcAUC(diab_lasso, diab_train)

#coefficients(diab_lasso$finalModel, s = diab_lasso$bestTune$lambda)

cm_lasso <- confusionMatrix(data = predict(diab_lasso, type = "raw", na.action = na.pass), #predictions
                reference = diab_train$readmitted30)#, #actuals

cm_lasso$byClass['Sensitivity']
```

Next, we used LASSO to construct a model. We tested a wide range of tuning parameter values and found the “best” lambda, which corresponded to the maximum accuracy, as shown in the plot (above). With lambda = 0.0351, most of the coefficients were shrunk to 0. Those that remained in the model include `discharge_disposition,` `number_inpatient,` `number_diagnoses,` and `time_in_hospital,` which were mentioned previously as variables found to be more influential. The sensitivity was 0.7180, which is higher than the logistic regression model and indicates that this model is better at correctly predicting which patients are readmitted within 30 days. The accuracy was better than the previous logistic model, while the AUC decreased a bit.


## Classification Tree
```{r, results = 'hide'}
# set.seed(253)
# 
# diabO_tree <- train(
#   readmitted30 ~ .,
#   data = other_subset,
#   method = "rpart",
#   tuneGrid = data.frame(cp = 10^seq(-4, -2 , 
#                                     length = 50)),
#   trControl = trainControl(method = "cv", number = 5, sampling = "down"),
#   metric = "Accuracy",
#   na.action = na.omit
# )
```

```{r, results = 'hide'}
# diabO_tree$bestTune
# diabO_tree
# 
# diabO_tree$results %>%
#   ggplot(aes(x = cp, y = Accuracy)) +
#   geom_line()+
#   geom_point()
```


```{r, echo = TRUE}
set.seed(253)

diab_tree <- train(
  readmitted30 ~ .,
  data = diab_train %>% select(-readmitted),
  method = "rpart",
  tuneGrid = data.frame(cp = 10^seq(-5, -2, # if you go out to -1 you see the no information rate
                                    length = 100)),
  trControl = trainControl(method = "cv", number = 5, sampling = "down"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r, fig.width=10}
rpart.plot(diab_tree$finalModel)
```

```{r}
diab_tree$results %>%
  ggplot(aes(x = cp, y = Accuracy)) +
  geom_line()+
  geom_point()

diab_tree$bestTune
diab_tree$results[75,]

calcAUC(diab_tree, diab_train)

cm_tree <- confusionMatrix(data = predict(diab_tree, type = "raw", na.action = na.pass), #predictions
                reference = diab_train$readmitted30)#, #actuals

cm_tree$byClass['Sensitivity']
```

We also built two hierarchical models: a classification tree and a random forest. We again tested a range of values for the tuning parameter, which for tree models is the complexity parameter (cp) (see above graph). We found a cp value that gave a maximal accuracy of 0.6296, which is lower than the previous two models. The sensitivity obtained with the classification tree was 0.6028, which is quite a bit smaller than the other two models. The AUC was slightly lower than for the previous models.

We visualized the results as a tree. Looking at this output, we can reaffirm that variables found to be more significant in the LASSO model, like `number_inpatient` and `discharge_disposition_id`, are important. The tree also includes other variables, like `metformin` and `admission_type`, which were not included in the LASSO model, but are still influential.


## Random Forest



```{r, echo = TRUE}
set.seed(327)
diab_randf <- train(
  readmitted30 ~ .,
  data = diab_train %>% select(-readmitted), 
  method = "rf",
  metric = "Accuracy",
  trControl = trainControl(method = "oob", sampling = "down"),
  tuneGrid = data.frame(mtry = c(2,4,6,8,10,12)),
  ntree = 50, #number of trees used, default is 500
  importance = TRUE, #for importance plots later
  nodesize = 5, #this is the default terminal node size for regression trees. Could set larger for smaller trees.
  na.action = na.omit
)
```


```{r}
# plot(diab_randf$finalModel)

calcAUC(diab_randf, diab_train)

cm_randf <- confusionMatrix(data = predict(diab_randf, diab_train, type = "raw"), #predictions
                reference = diab_train$readmitted30)#, #actuals

cm_randf$byClass['Sensitivity']

diab_randf$bestTune
diab_randf$results[3,]
```

```{r}
vip(diab_randf$finalModel)
```

For the random forest method we used 50 trees, after verifying that the error levelled out by this number. The tuning parameter of interest is mtry, which represents the number of variables considered at each split. We found that mtry = 6 corresponded to the highest accuracy. The sensitivity is 0.6768, which is the second highest sensitivity of all the models. Surprisingly, when compared to sensitivity, the accuracy and AUC were more disparate than for other models. The accuracy, 0.5979, was the lowest of all models and the AUC, 0.7894, was the highest. 

We also created an importance plot for the random forest to examine which variables were most significant in the model. From the plot, we again see that `number_inpatient`, `discharge_disposition_id`, and `number_emergency` are key variables.


# Evaluating the Models

First, we looked at the performance statistics for each models, built and evaluated with the training dataset: 

```{r}
x <- matrix(c(0.6569,0.7180,0.6028,0.6768,0.6423,0.7173,0.6296,0.5918,0.6583,0.6429,0.6398,0.7894), nrow = 4, dimnames = list(c("logisitic","lasso","classification tree", "random forest"), c("Sensitivity (TPR)","CV/OOB Accuracy","AUC")))
kable(x, align = c('ccc'))
```

In the context of our research question, we are most interested in the sensitivity of the model, since that is what ensures that high-risk patients are identified and can be monitored when they leave the hospital. In addition, we kept in mind overall accuracy and AUC to judge how the models did on the whole. Looking at the table, we see that the LASSO model has the best sensitivity and accuracy, while the classification tree has the lowest sensitivity, accuracy, and AUC. Meanwhile, the logistic model is mediocre in all categories and the random forest has a good sensitivity, poor accuracy, and very good AUC. As a result, we decided to move forward with the logistic, LASSO, and random forest models and see how well they perform on the test data.


```{r}
# function to compute accuracy 
calcAccuracy <- function(model, data){
  newData <- data %>%
    mutate(predRead = predict(model, newdata = data),
           diff = abs(as.numeric(readmitted30) - as.numeric(predRead)))
  1-(sum(as.numeric(newData$diff))/as.numeric(count(data)))
}

calcSensitivity <- function(model){
  cm <- confusionMatrix(data = predict(model, diab_test, type = "raw"), #predictions
                reference = diab_test$readmitted30)#, #actuals
  cm$byClass['Sensitivity']
}

```

The following table summarizes the computed sensitivity and accuracy of the models on the test data. We excluded AUC because our primary interest is sensitivity, and we felt that the accuracy was comprehensive enough to give us a complementary measure of how the model is doing on the entire test dataset.

```{r}

y <- matrix(c(calcSensitivity(diab_logAll),calcSensitivity(diab_lasso),calcSensitivity(diab_randf)
,calcAccuracy(diab_logAll, diab_test),calcAccuracy(diab_lasso, diab_test),calcAccuracy(diab_randf, diab_test)
), nrow = 3, dimnames = list(c("logisitic","lasso", "random forest"), c("Sensitivity (TPR)","Accuracy")))

kable(y, align = c('ccc'))
```

Comparing the results above, we see the LASSO model is clearly our best. It has the highest sensitivity, 0.7209, and the highest accuracy, 0.6920. The logistic and random forest models performed similarly on the test data and neither was close to the performance of the LASSO model.

# Conclusion

We were not able to develop a model that yielded a better prediction of hospital readmittance than the NIR, which was 0.886. However, we created a model using the LASSO technique with fairly high sensitivity and fewer variables. A model with high sensitivity and less predictor variables is still desirable because it is less complex and inaccessible to non-statisticians, while still allowing doctors to make predictions about which diabetes patients should be more closely monitored based on their likelihood to be readmitted within 30 days. In particular, we found that a patient’s previous hospital admittance history (`number_inpatient`) and where they were discharged to (`discharge_disposition_id`) were key in predicting whether they’d be readmitted soon. Additionally, the number of diagnoses a patient had (`number_diagnoses`) was a key variable, possibly because it indicates other conditions and the severity of their diabetes. Interestingly, the medication(s) they were put on did not seem to have much of an impact on readmittance. We tried a few models on a subset of the data composed only of predictor variables that were diabetes medications, expecting medications to be relatively powerful predictors, but were surprised to find that the best of these models only resulted in about 60% accuracy. Overall, the LASSO model’s high sensitivity could help pinpoint patients at high risk for readmittance before they leave the hospital, which will allow doctors to be proactive in caring for patients and preventing readmittance.





